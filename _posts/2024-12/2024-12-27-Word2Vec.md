---
title: Wrod2vec는 뭐든지 할 수 있다구요
date: 2024-12-30 14:30:55 +09:00
excerpt: "코딩"
categories: [프로그래밍, 파이썬,Word2vec]
toc: true
toc_sticky: true
---

# Word2vec

one hot encoding을 사용하면서도 단어 간 유사도를 반영할 수 있도록 단어의 의미를 벡터화하는 방법이다

<a style="color:red" href="https://ko.wikipedia.org/wiki/%EB%B6%84%ED%8F%AC_%EC%9D%98%EB%AF%B8%EB%A1%A0">비슷한 위치에서 등장하는 단어들은 비슷한 의미를 가진다</a>는 분포 가설을 따르는 분산 표현 방법을 사용한다.

![분포 가설 이미지](https://upload.wikimedia.org/wikipedia/commons/thumb/c/ce/Distributional_semantics.png/440px-Distributional_semantics.png)

<center style="color:gray">시멘틱 공간에 보여진 단어의 관련성</center>

<p style="color:#1ec997;"> '코딩'을 예시로 들자면 '기술','미래'등과 함께 사용하는데 이러한 내용을 가지는 텍스트를 벡터화하면 이 단어들은 의미가 가까운 단어가 된다 </p>

World2Vec에는 2개의 방식이 있다.

- CBOW

- Skip-Gram

## CBOW

CBOW란 Continous Bag of Words의 약자로 주변에 있는 단어들을 통해 중간에 있는 단어를 예측하는 방법입니다.

n을 정하여 n 주변의 단어의 벡터로 중심 단어의 벡터를 예측한다.

Skip-Gram방식보다 몇 배 빠른 훈련이 가능하며, 빈번한 단어를 찾은것에 최적화되어있다.



## Skip-Gram

Skip-Gram은 CBOW와 비슷한 매커니즘을 가지고 있습니다.

Skip-Gram은 중심 단어에서 주변 단어를 예측합니다.

중심 단어에 대해서 주변 단어를 예측함으로 투사층에서 벡터들의 평균을 구하는 과정은 없습니다.

